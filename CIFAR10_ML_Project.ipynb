{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 ML Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdhirajChaddha/CIFAR-10/blob/master/CIFAR10_ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8chO3HzFOAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8cb1c14-a2bc-4792-b307-5c19e8e0bbae"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "print(\"Training on GPU: \", train_on_gpu)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpH4yPHQFdFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c39a048d-db3a-4431-b4a8-62447bea41e6"
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "batchSize = 20\n",
        "validationSize = 0.2\n",
        "\n",
        "transformsTrain = transforms.Compose([\n",
        "#     transforms.Pad(1),\n",
        "#     transforms.RandomGrayscale(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transformsTest = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "# Download training and testing data\n",
        "trainingData = datasets.CIFAR10('data', train=True, \n",
        "                                transform=transformsTrain, download=True)\n",
        "\n",
        "testingData = datasets.CIFAR10('data', train=False, \n",
        "                                transform=transformsTest, download=True)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIjJLrpgGDUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Devide the data int0 training and validation sets\n",
        "length = len(trainingData)\n",
        "indices = list(range(length))\n",
        "np.random.shuffle(indices)\n",
        "split = (int)(length*validationSize)\n",
        "train_idx, val_idx = idices[:split], indices[split:]\n",
        "\n",
        "# Put the inices into the samplers for making dataloader\n",
        "trainSampler = SubsetRandomSampler(train_idx)\n",
        "validationSampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "# Create the data loaders\n",
        "trainLoader = torch.utils.data.DataLoader(trainingData, batch_size=batchSize,\n",
        "    sampler=trainSampler)\n",
        "valLoader = torch.utils.data.DataLoader(trainingData, batch_size=batchSize,\n",
        "    sampler=validationSampler)\n",
        "testLoader = torch.utils.data.DataLoader(testingData, batch_size=batchSize)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHXkt5I4UUsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfDveBSVWe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "dd9ad9ed-4b58-4d42-cc13-d29b9d8dc05a"
      },
      "source": [
        "dataiter = iter(trainLoader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(images.shape)\n",
        "\n",
        "imshow(images.cpu()[0])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBQFvxCJouAXIlEU/EIkyv8CvM/kQSvnJGIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLxOF6D6Vxlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "#     self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "#     self.conv11 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "#     self.conv21 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "#     self.conv31 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "    \n",
        "    self.maxPool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    self.fc1 = nn.Linear(4*4*256, 2000)\n",
        "    self.fc2 = nn.Linear(2000, 500)\n",
        "    self.fc3 = nn.Linear(500, 10)\n",
        "    \n",
        "    \n",
        "    self.dropout20 = nn.Dropout(0.2)\n",
        "    self.dropout30 = nn.Dropout(0.3)\n",
        "    self.dropout40 = nn.Dropout(0.4)\n",
        "    self.dropout50 = nn.Dropout(0.5)\n",
        "    \n",
        "  def forward(self, x):\n",
        "#     x = F.relu(self.conv1(x))\n",
        "#     x = self.maxPool(x)\n",
        "    \n",
        "#     x = F.relu(self.conv2(x))\n",
        "#     x = self.maxPool(x)\n",
        "    \n",
        "#     x = F.relu(self.conv3(x))\n",
        "#     x = self.maxPool(x)\n",
        "    \n",
        "#     x = F.relu(self.conv4(x))\n",
        "    \n",
        "#     x = x.view(-1, 4*4*128)\n",
        "    \n",
        "#     x = self.dropout(x)\n",
        "#     x = F.relu(self.fc1(x))\n",
        "#     x = self.dropout(x)\n",
        "#     x = self.fc2(x)\n",
        "\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.maxPool(x)\n",
        "    x = self.dropout20(x)\n",
        "    \n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.maxPool(x)\n",
        "    x = self.dropout20(x)\n",
        "    \n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.maxPool(x)\n",
        "    x = self.dropout20(x)\n",
        "    \n",
        "#     x = F.relu(self.conv4(x))\n",
        "    \n",
        "    x = x.view(-1, 4*4*256)\n",
        "    \n",
        "    x = self.dropout20(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout20(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout20(x)\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "  \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNkat3c7YPSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNN()\n",
        "\n",
        "if train_on_gpu:\n",
        "  model.cuda()\n",
        "  \n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "LossFunction = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnaz4s_8YRI5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "205f1520-be04-492a-c1ea-f920f6275c6d"
      },
      "source": [
        "epochs = 150\n",
        "\n",
        "# Set the minimum validation loss achieved to infinity innitailly \n",
        "minimumValidationLoss = np.Inf\n",
        "\n",
        "valLosses = []\n",
        "trainLosses = []\n",
        "  \n",
        "for e in range(epochs):\n",
        "  trainLoss = 0.0\n",
        "  validationLoss = 0.0\n",
        "  \n",
        "  ################# \n",
        "  # Training Loop #\n",
        "  #################\n",
        "  model.train()\n",
        "  for images, labels in trainLoader:\n",
        "    if train_on_gpu:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # Set gradients to zero from last step\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get output, calculate loss, backpropogate and then update the weights\n",
        "    output = model(images) # output\n",
        "    loss = LossFunction(output, labels) # loss calcluation \n",
        "    loss.backward() # backpropogation\n",
        "    optimizer.step() # updating weights\n",
        "    \n",
        "    # Get the total loss over the batch by multiplying loss with batch size\n",
        "    trainLoss += loss.item()*images.size(0) \n",
        "    \n",
        "  ###################\n",
        "  # Validation Loop #\n",
        "  ###################\n",
        "  model.eval()\n",
        "  for images, labels in valLoader:\n",
        "    if train_on_gpu:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # Get output, calculate loss, backpropogate and then update the weights\n",
        "    output = model(images) # output\n",
        "    loss = LossFunction(output, labels) # loss calcluation \n",
        "    \n",
        "    # Get the total loss over the batch by multiplying loss with batch size\n",
        "    validationLoss += loss.item()*images.size(0) \n",
        "    \n",
        "    \n",
        "  # Getting the average losses in trainin gand validation loop\n",
        "  avgTrainLoss = trainLoss/len(trainLoader.dataset)  \n",
        "  avgValLoss = validationLoss/len(valLoader.dataset)\n",
        "  \n",
        "  valLosses.append(avgValLoss/4)\n",
        "  trainLosses.append(avgTrainLoss)\n",
        "  \n",
        "  \n",
        "  print(\"Epoch: \", e+1, '\\tTraining Loss: ', \n",
        "        avgTrainLoss, '\\tValidation Loss: ', avgValLoss)\n",
        "  \n",
        "  # Saving the model and setting new minValidationLoss if current model has \n",
        "  # lower loss that the previous one saved\n",
        "  if avgValLoss <= minimumValidationLoss:\n",
        "    print(\"Saving model \\t Validation loss gone from: \", minimumValidationLoss,\n",
        "         \" to: \", avgValLoss)\n",
        "    torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "    minimumValidationLoss = avgValLoss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 \tTraining Loss:  0.46047977714538574 \tValidation Loss:  1.8416484498023986\n",
            "Saving model \t Validation loss gone from:  inf  to:  1.8416484498023986\n",
            "Epoch:  2 \tTraining Loss:  0.4603260976791382 \tValidation Loss:  1.8408886900901795\n",
            "Saving model \t Validation loss gone from:  1.8416484498023986  to:  1.8408886900901795\n",
            "Epoch:  3 \tTraining Loss:  0.46003488330841064 \tValidation Loss:  1.8394019198417664\n",
            "Saving model \t Validation loss gone from:  1.8408886900901795  to:  1.8394019198417664\n",
            "Epoch:  4 \tTraining Loss:  0.45939460821151734 \tValidation Loss:  1.8351883615493774\n",
            "Saving model \t Validation loss gone from:  1.8394019198417664  to:  1.8351883615493774\n",
            "Epoch:  5 \tTraining Loss:  0.4565857162475586 \tValidation Loss:  1.8125126560211182\n",
            "Saving model \t Validation loss gone from:  1.8351883615493774  to:  1.8125126560211182\n",
            "Epoch:  6 \tTraining Loss:  0.44402801179885865 \tValidation Loss:  1.7495832871437074\n",
            "Saving model \t Validation loss gone from:  1.8125126560211182  to:  1.7495832871437074\n",
            "Epoch:  7 \tTraining Loss:  0.4325478374004364 \tValidation Loss:  1.7184817954540252\n",
            "Saving model \t Validation loss gone from:  1.7495832871437074  to:  1.7184817954540252\n",
            "Epoch:  8 \tTraining Loss:  0.4263243032932281 \tValidation Loss:  1.6954543216228486\n",
            "Saving model \t Validation loss gone from:  1.7184817954540252  to:  1.6954543216228486\n",
            "Epoch:  9 \tTraining Loss:  0.42159307627677917 \tValidation Loss:  1.679146946287155\n",
            "Saving model \t Validation loss gone from:  1.6954543216228486  to:  1.679146946287155\n",
            "Epoch:  10 \tTraining Loss:  0.4182156973361969 \tValidation Loss:  1.6694034748077393\n",
            "Saving model \t Validation loss gone from:  1.679146946287155  to:  1.6694034748077393\n",
            "Epoch:  11 \tTraining Loss:  0.4149613107681274 \tValidation Loss:  1.654594482088089\n",
            "Saving model \t Validation loss gone from:  1.6694034748077393  to:  1.654594482088089\n",
            "Epoch:  12 \tTraining Loss:  0.41280416159629824 \tValidation Loss:  1.6445454469203948\n",
            "Saving model \t Validation loss gone from:  1.654594482088089  to:  1.6445454469203948\n",
            "Epoch:  13 \tTraining Loss:  0.4111175706863403 \tValidation Loss:  1.638998945760727\n",
            "Saving model \t Validation loss gone from:  1.6445454469203948  to:  1.638998945760727\n",
            "Epoch:  14 \tTraining Loss:  0.4092475240707397 \tValidation Loss:  1.6347080766201019\n",
            "Saving model \t Validation loss gone from:  1.638998945760727  to:  1.6347080766201019\n",
            "Epoch:  15 \tTraining Loss:  0.40738883571624757 \tValidation Loss:  1.625514465522766\n",
            "Saving model \t Validation loss gone from:  1.6347080766201019  to:  1.625514465522766\n",
            "Epoch:  16 \tTraining Loss:  0.40602608246803285 \tValidation Loss:  1.616602215719223\n",
            "Saving model \t Validation loss gone from:  1.625514465522766  to:  1.616602215719223\n",
            "Epoch:  17 \tTraining Loss:  0.40377050457000735 \tValidation Loss:  1.6183652019500732\n",
            "Epoch:  18 \tTraining Loss:  0.40231505546569823 \tValidation Loss:  1.6078491481781005\n",
            "Saving model \t Validation loss gone from:  1.616602215719223  to:  1.6078491481781005\n",
            "Epoch:  19 \tTraining Loss:  0.40068437724113465 \tValidation Loss:  1.5951542155742646\n",
            "Saving model \t Validation loss gone from:  1.6078491481781005  to:  1.5951542155742646\n",
            "Epoch:  20 \tTraining Loss:  0.3989927007675171 \tValidation Loss:  1.5958427570343017\n",
            "Epoch:  21 \tTraining Loss:  0.39796052346229555 \tValidation Loss:  1.587758502960205\n",
            "Saving model \t Validation loss gone from:  1.5951542155742646  to:  1.587758502960205\n",
            "Epoch:  22 \tTraining Loss:  0.3968890331745148 \tValidation Loss:  1.5849512671947479\n",
            "Saving model \t Validation loss gone from:  1.587758502960205  to:  1.5849512671947479\n",
            "Epoch:  23 \tTraining Loss:  0.39530163159370424 \tValidation Loss:  1.5850402455329895\n",
            "Epoch:  24 \tTraining Loss:  0.3937303825855255 \tValidation Loss:  1.5746673284053803\n",
            "Saving model \t Validation loss gone from:  1.5849512671947479  to:  1.5746673284053803\n",
            "Epoch:  25 \tTraining Loss:  0.39353583278656007 \tValidation Loss:  1.5699644728660584\n",
            "Saving model \t Validation loss gone from:  1.5746673284053803  to:  1.5699644728660584\n",
            "Epoch:  26 \tTraining Loss:  0.3927701206207275 \tValidation Loss:  1.5687305844783783\n",
            "Saving model \t Validation loss gone from:  1.5699644728660584  to:  1.5687305844783783\n",
            "Epoch:  27 \tTraining Loss:  0.391328758764267 \tValidation Loss:  1.5678020933151244\n",
            "Saving model \t Validation loss gone from:  1.5687305844783783  to:  1.5678020933151244\n",
            "Epoch:  28 \tTraining Loss:  0.39070794491767885 \tValidation Loss:  1.5588417663574219\n",
            "Saving model \t Validation loss gone from:  1.5678020933151244  to:  1.5588417663574219\n",
            "Epoch:  29 \tTraining Loss:  0.38955748410224916 \tValidation Loss:  1.5564277217388154\n",
            "Saving model \t Validation loss gone from:  1.5588417663574219  to:  1.5564277217388154\n",
            "Epoch:  30 \tTraining Loss:  0.3886916603565216 \tValidation Loss:  1.5595742197036744\n",
            "Epoch:  31 \tTraining Loss:  0.3880531124591827 \tValidation Loss:  1.5506585636615753\n",
            "Saving model \t Validation loss gone from:  1.5564277217388154  to:  1.5506585636615753\n",
            "Epoch:  32 \tTraining Loss:  0.38764066252708435 \tValidation Loss:  1.5432314604759216\n",
            "Saving model \t Validation loss gone from:  1.5506585636615753  to:  1.5432314604759216\n",
            "Epoch:  33 \tTraining Loss:  0.3863992023944855 \tValidation Loss:  1.54200486369133\n",
            "Saving model \t Validation loss gone from:  1.5432314604759216  to:  1.54200486369133\n",
            "Epoch:  34 \tTraining Loss:  0.38591189155578615 \tValidation Loss:  1.5413838272571563\n",
            "Saving model \t Validation loss gone from:  1.54200486369133  to:  1.5413838272571563\n",
            "Epoch:  35 \tTraining Loss:  0.38502511367797854 \tValidation Loss:  1.540805401945114\n",
            "Saving model \t Validation loss gone from:  1.5413838272571563  to:  1.540805401945114\n",
            "Epoch:  36 \tTraining Loss:  0.38416951599121096 \tValidation Loss:  1.5372043667316437\n",
            "Saving model \t Validation loss gone from:  1.540805401945114  to:  1.5372043667316437\n",
            "Epoch:  37 \tTraining Loss:  0.3836396104812622 \tValidation Loss:  1.5295349286079407\n",
            "Saving model \t Validation loss gone from:  1.5372043667316437  to:  1.5295349286079407\n",
            "Epoch:  38 \tTraining Loss:  0.38265228576660154 \tValidation Loss:  1.5275555163383483\n",
            "Saving model \t Validation loss gone from:  1.5295349286079407  to:  1.5275555163383483\n",
            "Epoch:  39 \tTraining Loss:  0.38250513005256653 \tValidation Loss:  1.5290350439548492\n",
            "Epoch:  40 \tTraining Loss:  0.38169018263816834 \tValidation Loss:  1.527956973695755\n",
            "Epoch:  41 \tTraining Loss:  0.380512263250351 \tValidation Loss:  1.5246968789100648\n",
            "Saving model \t Validation loss gone from:  1.5275555163383483  to:  1.5246968789100648\n",
            "Epoch:  42 \tTraining Loss:  0.3802423858165741 \tValidation Loss:  1.5210978203773498\n",
            "Saving model \t Validation loss gone from:  1.5246968789100648  to:  1.5210978203773498\n",
            "Epoch:  43 \tTraining Loss:  0.3796455471038818 \tValidation Loss:  1.513536910867691\n",
            "Saving model \t Validation loss gone from:  1.5210978203773498  to:  1.513536910867691\n",
            "Epoch:  44 \tTraining Loss:  0.37890405259132387 \tValidation Loss:  1.5118132571220397\n",
            "Saving model \t Validation loss gone from:  1.513536910867691  to:  1.5118132571220397\n",
            "Epoch:  45 \tTraining Loss:  0.37833385515213014 \tValidation Loss:  1.5139830328941346\n",
            "Epoch:  46 \tTraining Loss:  0.3774351185321808 \tValidation Loss:  1.5160201499938966\n",
            "Epoch:  47 \tTraining Loss:  0.3771659945011139 \tValidation Loss:  1.5088678738594055\n",
            "Saving model \t Validation loss gone from:  1.5118132571220397  to:  1.5088678738594055\n",
            "Epoch:  48 \tTraining Loss:  0.377029816865921 \tValidation Loss:  1.5064815337657929\n",
            "Saving model \t Validation loss gone from:  1.5088678738594055  to:  1.5064815337657929\n",
            "Epoch:  49 \tTraining Loss:  0.3765083790779114 \tValidation Loss:  1.5037609647274017\n",
            "Saving model \t Validation loss gone from:  1.5064815337657929  to:  1.5037609647274017\n",
            "Epoch:  50 \tTraining Loss:  0.37571906661987303 \tValidation Loss:  1.506871435213089\n",
            "Epoch:  51 \tTraining Loss:  0.3755052958011627 \tValidation Loss:  1.5013004869937896\n",
            "Saving model \t Validation loss gone from:  1.5037609647274017  to:  1.5013004869937896\n",
            "Epoch:  52 \tTraining Loss:  0.37536914701461793 \tValidation Loss:  1.5033513511180878\n",
            "Epoch:  53 \tTraining Loss:  0.37441472063064574 \tValidation Loss:  1.497966675043106\n",
            "Saving model \t Validation loss gone from:  1.5013004869937896  to:  1.497966675043106\n",
            "Epoch:  54 \tTraining Loss:  0.3742555555343628 \tValidation Loss:  1.4949918140411378\n",
            "Saving model \t Validation loss gone from:  1.497966675043106  to:  1.4949918140411378\n",
            "Epoch:  55 \tTraining Loss:  0.3735133043766022 \tValidation Loss:  1.498067138671875\n",
            "Epoch:  56 \tTraining Loss:  0.3733404185771942 \tValidation Loss:  1.493495892906189\n",
            "Saving model \t Validation loss gone from:  1.4949918140411378  to:  1.493495892906189\n",
            "Epoch:  57 \tTraining Loss:  0.37291164088249207 \tValidation Loss:  1.4892721256256103\n",
            "Saving model \t Validation loss gone from:  1.493495892906189  to:  1.4892721256256103\n",
            "Epoch:  58 \tTraining Loss:  0.3720875382900238 \tValidation Loss:  1.4928282242774964\n",
            "Epoch:  59 \tTraining Loss:  0.37253183970451353 \tValidation Loss:  1.4881619037151337\n",
            "Saving model \t Validation loss gone from:  1.4892721256256103  to:  1.4881619037151337\n",
            "Epoch:  60 \tTraining Loss:  0.37131907052993773 \tValidation Loss:  1.4871600163936616\n",
            "Saving model \t Validation loss gone from:  1.4881619037151337  to:  1.4871600163936616\n",
            "Epoch:  61 \tTraining Loss:  0.37137922897338865 \tValidation Loss:  1.4880810407161713\n",
            "Epoch:  62 \tTraining Loss:  0.3714788522720337 \tValidation Loss:  1.4860798627376557\n",
            "Saving model \t Validation loss gone from:  1.4871600163936616  to:  1.4860798627376557\n",
            "Epoch:  63 \tTraining Loss:  0.3709770264625549 \tValidation Loss:  1.484210038280487\n",
            "Saving model \t Validation loss gone from:  1.4860798627376557  to:  1.484210038280487\n",
            "Epoch:  64 \tTraining Loss:  0.37003409967422485 \tValidation Loss:  1.4834956521511078\n",
            "Saving model \t Validation loss gone from:  1.484210038280487  to:  1.4834956521511078\n",
            "Epoch:  65 \tTraining Loss:  0.36951536483764647 \tValidation Loss:  1.480128808450699\n",
            "Saving model \t Validation loss gone from:  1.4834956521511078  to:  1.480128808450699\n",
            "Epoch:  66 \tTraining Loss:  0.3691822014808655 \tValidation Loss:  1.4791193850517272\n",
            "Saving model \t Validation loss gone from:  1.480128808450699  to:  1.4791193850517272\n",
            "Epoch:  67 \tTraining Loss:  0.369296130657196 \tValidation Loss:  1.4808748496055604\n",
            "Epoch:  68 \tTraining Loss:  0.36882198786735537 \tValidation Loss:  1.4788585334777833\n",
            "Saving model \t Validation loss gone from:  1.4791193850517272  to:  1.4788585334777833\n",
            "Epoch:  69 \tTraining Loss:  0.3686019980430603 \tValidation Loss:  1.4776722926139831\n",
            "Saving model \t Validation loss gone from:  1.4788585334777833  to:  1.4776722926139831\n",
            "Epoch:  70 \tTraining Loss:  0.36771772627830507 \tValidation Loss:  1.4814337252616883\n",
            "Epoch:  71 \tTraining Loss:  0.367410467004776 \tValidation Loss:  1.4742944074630737\n",
            "Saving model \t Validation loss gone from:  1.4776722926139831  to:  1.4742944074630737\n",
            "Epoch:  72 \tTraining Loss:  0.3672615618228912 \tValidation Loss:  1.478303113603592\n",
            "Epoch:  73 \tTraining Loss:  0.3674687738418579 \tValidation Loss:  1.4741782749176024\n",
            "Saving model \t Validation loss gone from:  1.4742944074630737  to:  1.4741782749176024\n",
            "Epoch:  74 \tTraining Loss:  0.3669006951332092 \tValidation Loss:  1.473165421295166\n",
            "Saving model \t Validation loss gone from:  1.4741782749176024  to:  1.473165421295166\n",
            "Epoch:  75 \tTraining Loss:  0.3665738585948944 \tValidation Loss:  1.4751882227897644\n",
            "Epoch:  76 \tTraining Loss:  0.36639160513877866 \tValidation Loss:  1.470430499458313\n",
            "Saving model \t Validation loss gone from:  1.473165421295166  to:  1.470430499458313\n",
            "Epoch:  77 \tTraining Loss:  0.3663754852294922 \tValidation Loss:  1.46862029004097\n",
            "Saving model \t Validation loss gone from:  1.470430499458313  to:  1.46862029004097\n",
            "Epoch:  78 \tTraining Loss:  0.3656343883514404 \tValidation Loss:  1.4687066098213195\n",
            "Epoch:  79 \tTraining Loss:  0.365275847864151 \tValidation Loss:  1.4675497592926026\n",
            "Saving model \t Validation loss gone from:  1.46862029004097  to:  1.4675497592926026\n",
            "Epoch:  80 \tTraining Loss:  0.365250333404541 \tValidation Loss:  1.4658156202793122\n",
            "Saving model \t Validation loss gone from:  1.4675497592926026  to:  1.4658156202793122\n",
            "Epoch:  81 \tTraining Loss:  0.36489141297340394 \tValidation Loss:  1.4650390998363494\n",
            "Saving model \t Validation loss gone from:  1.4658156202793122  to:  1.4650390998363494\n",
            "Epoch:  82 \tTraining Loss:  0.36449177269935606 \tValidation Loss:  1.4621257900238036\n",
            "Saving model \t Validation loss gone from:  1.4650390998363494  to:  1.4621257900238036\n",
            "Epoch:  83 \tTraining Loss:  0.36353280262947085 \tValidation Loss:  1.46767648730278\n",
            "Epoch:  84 \tTraining Loss:  0.3636197904586792 \tValidation Loss:  1.4638237293720244\n",
            "Epoch:  85 \tTraining Loss:  0.3634294012069702 \tValidation Loss:  1.4614405595302582\n",
            "Saving model \t Validation loss gone from:  1.4621257900238036  to:  1.4614405595302582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgheqFxKeE5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "346dc35f-845d-4e8d-92e0-4f982f34e517"
      },
      "source": [
        "model.load_state_dict(torch.load('model_cifar.pt'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = list(range(epochs))\n",
        "plt.plot(x, valLosses, label = 'validation')\n",
        "plt.plot(x, trainLosses, label = 'train')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0qIeGxqg5sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "255a5001-7dde-4c1b-95ba-b0bee73e669c"
      },
      "source": [
        "testLoss = 0.0\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "for images, labels in testLoader:\n",
        "  if train_on_gpu:\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "      \n",
        "  out = model(images)\n",
        "  loss = LossFunction(out, labels)\n",
        "  \n",
        "  testLoss += loss.item()*images.size(0)\n",
        "  \n",
        "  _, pred = torch.max(out, 1)    \n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  # calculate test accuracy for each object class\n",
        "  for i in range(batchSize):\n",
        "    label = labels.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "  \n",
        "  \n",
        "testLoss = testLoss/len(testLoader.dataset)\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "print(\"Avg Loss is: \", testLoss)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of airplane: 50% (503/1000)\n",
            "Test Accuracy of automobile: 81% (817/1000)\n",
            "Test Accuracy of  bird: 39% (396/1000)\n",
            "Test Accuracy of   cat: 57% (576/1000)\n",
            "Test Accuracy of  deer: 47% (470/1000)\n",
            "Test Accuracy of   dog: 40% (402/1000)\n",
            "Test Accuracy of  frog: 82% (825/1000)\n",
            "Test Accuracy of horse: 61% (613/1000)\n",
            "Test Accuracy of  ship: 81% (815/1000)\n",
            "Test Accuracy of truck: 66% (664/1000)\n",
            "\n",
            "Test Accuracy (Overall): 60% (6081/10000)\n",
            "Avg Loss is:  1.1062940104007721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i715l2OQhd2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}